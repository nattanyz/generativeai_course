{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic Model List - Jupyter Notebook\n",
    "\n",
    "This notebook demonstrates how to retrieve and display a list of available models from the Anthropic AI platform.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this code, make sure you have:\n",
    "\n",
    "1. Installed the Anthropic Python client:\n",
    "   ```\n",
    "   !pip install anthropic\n",
    "   ```\n",
    "\n",
    "2. Set your Anthropic API key as an environment variable:\n",
    "   ```\n",
    "   import os\n",
    "   os.environ[\"ANTHROPIC_API_KEY\"] = \"your_api_key_here\"  # Replace with your actual API key\n",
    "   ```\n",
    "   \n",
    "   Alternatively, you can create a `.env` file and load it:\n",
    "   ```\n",
    "   from dotenv import load_dotenv\n",
    "   load_dotenv()\n",
    "   ```\n",
    "\n",
    "## Retrieving and Displaying Anthropic Models\n",
    "\n",
    "The code below will:\n",
    "1. Initialize the Anthropic client\n",
    "2. Retrieve a list of available models (limited to 20)\n",
    "3. Display each model's ID and display name in a formatted way\n",
    "\n",
    "```python\n",
    "# Import the Anthropic library\n",
    "import anthropic\n",
    "\n",
    "# Initialize the client\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Get the models\n",
    "models = client.models.list(limit=20)\n",
    "\n",
    "# Print model ID and display name\n",
    "print(\"Available Anthropic Models:\")\n",
    "print(\"--------------------------\")\n",
    "for model in models.data:\n",
    "    print(f\"{model.id:<35} | {model.display_name}\")\n",
    "```\n",
    "\n",
    "## How It Works\n",
    "\n",
    "- The script uses the official Anthropic Python client to interact with their API\n",
    "- We limit the results to 20 models, although Anthropic typically offers fewer than that\n",
    "- For each model, we display:\n",
    "  - The model ID (e.g., \"claude-3-7-sonnet-20250219\") - this is what you use in API calls\n",
    "  - The human-readable display name (e.g., \"Claude 3.7 Sonnet\")\n",
    "- The `:<35` in the f-string is a formatting instruction that left-aligns the model ID text and pads it to 35 characters, creating a clean output\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "When run, this code should produce output similar to:\n",
    "\n",
    "```\n",
    "Available Anthropic Models:\n",
    "--------------------------\n",
    "claude-3-7-sonnet-20250219         | Claude 3.7 Sonnet\n",
    "claude-3-5-sonnet-20240620         | Claude 3.5 Sonnet\n",
    "claude-3-opus-20240229             | Claude 3 Opus\n",
    "claude-3-sonnet-20240229           | Claude 3 Sonnet\n",
    "claude-2.1                         | Claude 2.1\n",
    "```\n",
    "\n",
    "## Enhanced Display (Optional)\n",
    "\n",
    "If you want a more visually appealing display, you can use the `tabulate` library:\n",
    "\n",
    "```python\n",
    "import anthropic\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Initialize the client\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Get the models\n",
    "models = client.models.list(limit=20)\n",
    "\n",
    "# Prepare data for tabulation\n",
    "model_data = []\n",
    "for model in models.data:\n",
    "    # Format the date to be more readable\n",
    "    created_date = model.created_at.strftime(\"%Y-%m-%d\")\n",
    "    model_data.append([model.id, model.display_name, created_date])\n",
    "\n",
    "# Print as a table\n",
    "print(tabulate(model_data, headers=[\"Model ID\", \"Display Name\", \"Created Date\"], tablefmt=\"grid\"))\n",
    "```\n",
    "\n",
    "Make sure to install tabulate first with `!pip install tabulate` if you choose this option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Anthropic Models:\n",
      "--------------------------\n",
      "claude-opus-4-20250514              | Claude Opus 4\n",
      "claude-sonnet-4-20250514            | Claude Sonnet 4\n",
      "claude-3-7-sonnet-20250219          | Claude Sonnet 3.7\n",
      "claude-3-5-sonnet-20241022          | Claude Sonnet 3.5 (New)\n",
      "claude-3-5-haiku-20241022           | Claude Haiku 3.5\n",
      "claude-3-5-sonnet-20240620          | Claude Sonnet 3.5 (Old)\n",
      "claude-3-haiku-20240307             | Claude Haiku 3\n",
      "claude-3-opus-20240229              | Claude Opus 3\n",
      "claude-3-sonnet-20240229            | Claude Sonnet 3\n",
      "claude-2.1                          | Claude 2.1\n",
      "claude-2.0                          | Claude 2.0\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Initialize the client\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Get the models\n",
    "models = client.models.list(limit=20)\n",
    "\n",
    "# Print model ID and display name\n",
    "print(\"Available Anthropic Models:\")\n",
    "print(\"--------------------------\")\n",
    "for model in models.data:\n",
    "    print(f\"{model.id:<35} | {model.display_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API keys are successfully loaded.\n",
      "ðŸ”’ API keys are loaded but hidden for security.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Check if API keys are loaded\n",
    "if anthropic_api_key:\n",
    "    print(\"âœ… API keys are successfully loaded.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: One or more API keys are missing.\")\n",
    "\n",
    "# Optionally, display API keys (for debugging purposes only)\n",
    "display_keys = False  # Change to True if you want to see the keys\n",
    "\n",
    "if display_keys:\n",
    "    print(f\"Anthropic API Key: {anthropic_api_key}\")\n",
    "else:\n",
    "    print(\"ðŸ”’ API keys are loaded but hidden for security.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"claude-3-5-haiku-20241022\"\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "def get_completion(prompt: str):\n",
    "    message = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Hello, Claude!\"\n",
    "\n",
    "# Get Claude's response\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Model Comparison Guide\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "Anthropic offers several Claude models, each optimized for different use cases. Here's a comprehensive comparison to help you choose the right model for your needs. This is a rough guide to help you think about pricing. \n",
    "\n",
    "### Claude 4 Series (Latest)\n",
    "\n",
    "| Model | Context Window | Strengths | Best For | Pricing (per M tokens) |\n",
    "|-------|----------------|-----------|----------|------------------------|\n",
    "| **Claude Opus 4** | 200K tokens | Most capable, advanced reasoning, complex tasks | Research, analysis, coding, creative writing | Input: $15, Output: $75 |\n",
    "| **Claude Sonnet 4** | 200K tokens | Balanced performance and speed | General purpose, business applications | Input: $3, Output: $15 |\n",
    "\n",
    "### Claude 3 Series\n",
    "\n",
    "| Model | Context Window | Strengths | Best For | Pricing (per M tokens) |\n",
    "|-------|----------------|-----------|----------|------------------------|\n",
    "| **Claude 3.7 Sonnet** | 200K tokens | Enhanced reasoning, improved coding | Development, technical writing | Input: $3, Output: $15 |\n",
    "| **Claude 3.5 Sonnet (New)** | 200K tokens | Fast, versatile, good at reasoning | Most general use cases | Input: $3, Output: $15 |\n",
    "| **Claude 3.5 Haiku** | 200K tokens | Fastest, cost-effective | Simple tasks, high-volume processing | Input: $0.25, Output: $1.25 |\n",
    "| **Claude 3.5 Sonnet (Old)** | 200K tokens | Previous generation Sonnet | Legacy applications | Input: $3, Output: $15 |\n",
    "| **Claude 3 Opus** | 200K tokens | Most capable 3.x model | Complex reasoning, research | Input: $15, Output: $75 |\n",
    "| **Claude 3 Sonnet** | 200K tokens | Balanced 3.x model | General purpose | Input: $3, Output: $15 |\n",
    "| **Claude 3 Haiku** | 200K tokens | Fastest 3.x model | Simple, quick tasks | Input: $0.25, Output: $1.25 |\n",
    "\n",
    "## Model Selection Guide\n",
    "\n",
    "### ðŸŽ¯ **For Production Applications**\n",
    "- **Claude Sonnet 4**: Best balance of capability and cost\n",
    "- **Claude 3.5 Sonnet (New)**: Proven performance, widely adopted\n",
    "\n",
    "### âš¡ **For High-Volume/Speed-Critical Tasks**\n",
    "- **Claude 3.5 Haiku**: Fastest response times, most cost-effective\n",
    "- **Claude 3 Haiku**: Alternative for legacy systems\n",
    "\n",
    "### ðŸ§  **For Complex Reasoning & Analysis**\n",
    "- **Claude Opus 4**: Cutting-edge capabilities\n",
    "- **Claude 3 Opus**: Proven complex reasoning abilities\n",
    "\n",
    "### ðŸ’» **For Coding & Technical Tasks**\n",
    "- **Claude 3.7 Sonnet**: Enhanced for development workflows\n",
    "- **Claude Sonnet 4**: Advanced coding capabilities\n",
    "\n",
    "### ðŸ“ **For Creative Writing & Content**\n",
    "- **Claude Opus 4**: Most creative and nuanced\n",
    "- **Claude 3.5 Sonnet**: Good creative balance\n",
    "\n",
    "## Key Considerations\n",
    "\n",
    "### **Context Window**\n",
    "- All current models support 200K tokens (~150K words)\n",
    "- Ideal for processing long documents, codebases, or conversations\n",
    "\n",
    "### **Response Quality vs Speed**\n",
    "- **Opus models**: Highest quality, slower responses\n",
    "- **Sonnet models**: Balanced quality and speed\n",
    "- **Haiku models**: Fastest responses, good quality for simpler tasks\n",
    "\n",
    "### **Cost Optimization**\n",
    "- Start with **Claude 3.5 Haiku** for prototyping\n",
    "- Upgrade to **Sonnet** models for production\n",
    "- Use **Opus** models only when maximum capability is required\n",
    "\n",
    "### **Model Updates**\n",
    "- Claude 4 series represents the latest generation\n",
    "- Claude 3.5 models receive periodic updates\n",
    "- Always test new models before switching production systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages format\n",
    "\n",
    "As we saw in the previous lesson, we can use `client.messages.create()` to send a message to Claude and get a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_014WE4hqyZBF7Ynw3eTG39SF', content=[TextBlock(text='The exact recipe for Coca-Cola is a closely guarded trade secret, but the general flavors and ingredients are known:\\n\\n- Carbonated water - This provides the bubbly carbonation.\\n\\n- Caffeine - Coca-Cola contains around 34mg of caffeine per 12oz serving.\\n\\n- Sugar (or high fructose corn syrup) - This provides the sweetness.\\n\\n- Caramel coloring - This gives Coca-Cola its distinctive brown color.\\n\\n- Phosphoric acid - This adds tartness and a slightly acidic flavor.\\n\\nThe specific flavors used are a blend of citrus flavors (such as lemon, lime, and orange), spices (such as cinnamon and nutmeg), and other flavorings. The original Coca-Cola formula also contained coca leaf extract, which provided a small amount of cocaine, but this has been removed since the early 1900s.\\n\\nThe exact blend of these flavors is what gives Coca-Cola its unique taste, and the full recipe has been kept secret by the Coca-Cola company since its creation in 1886.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=17, output_tokens=247, service_tier='standard'))\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Coca Cola?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this bit: \n",
    "```py\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Dr. Pepper?\"}\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The messages parameter is a crucial part of interacting with the Claude API. It allows you to provide the conversation history and context for Claude to generate a relevant response. \n",
    "\n",
    "The messages parameter expects a list of message dictionaries, where each dictionary represents a single message in the conversation.\n",
    "Each message dictionary should have the following keys:\n",
    "\n",
    "* `role`: A string indicating the role of the message sender. It can be either \"user\" (for messages sent by the user) or \"assistant\" (for messages sent by Claude).\n",
    "* `content`: A string or list of content dictionaries representing the actual content of the message. If a string is provided, it will be treated as a single text content block. If a list of content dictionaries is provided, each dictionary should have a \"type\" (e.g., \"text\" or \"image\") and the corresponding content.  For now, we'll leave `content` as a single string.\n",
    "\n",
    "Here's an example of a messages list with a single user message:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude! How are you today?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "And here's an example with multiple messages representing a conversation:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude! How are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! I'm doing well, thank you. How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me a fun fact about ferrets?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Did you know that excited ferrets make a clucking vocalization known as 'dooking'?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Remember that messages always alternate between user and assistant messages (Source of Image: Anthropic Courses).\n",
    "\n",
    "![Alternating Messages](images/alternating_messages.png)\n",
    "\n",
    "The messages format allows us to structure our API calls to Claude in the form of a conversation, allowing for **context preservation**: The messages format allows for maintaining an entire conversation history, including both user and assistant messages. This ensures that Claude has access to the full context of the conversation when generating responses, leading to more coherent and relevant outputs.  \n",
    "\n",
    "**Note: many use-cases don't require a conversation history, and there's nothing wrong with providing a list of messages that only contains a single message!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Inspecting the message response\n",
    "Next, let's take a look at the shape of the response we get back from Claude. \n",
    "\n",
    "Let's ask Claude to do something simple and now let's inspect the contents of the `response` that we get back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01WapibPPHkyoxMZYvJAEwQ8', content=[TextBlock(text='Merci.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=20, output_tokens=7, service_tier='standard'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Translate Thank You to French. Respond with a single word\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back a `Message` object that contains a handful of properties.  Here's an example:\n",
    "\n",
    "```\n",
    "Message(id='msg_01Mq5gDnUmDESukTgwPV8xtG', content=[TextBlock(text='Bonjour.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=19, output_tokens=8))\n",
    "```\n",
    "\n",
    " The most important piece of information is the `content` property: this contains the actual content the model generated for us.   This is a **list** of content blocks, each of which has a type that determines its shape.\n",
    "\n",
    " In order to access the actual text content of the model's response, we need to do the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merci.\n"
     ]
    }
   ],
   "source": [
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `content`, the `Message` object contains some other pieces of information:\n",
    "\n",
    "* `id` - a unique object identifier\n",
    "* `type` - The object type, which will always be \"message\"\n",
    "* `role` - The conversational role of the generated message. This will always be \"assistant\".\n",
    "* `model` - The model that handled the request and generated the response\n",
    "* `stop_reason` - The reason the model stopped generating.  We'll learn more about this later.\n",
    "* `stop_sequence` - We'll learn more about this shortly.\n",
    "* `usage` - information on billing and rate-limit usage. Contains information on:\n",
    "    * `input_tokens` - The number of input tokens that were used.\n",
    "    * `output_tokens` - The number of output tokens that were used.\n",
    "\n",
    "It's important to know that we have access to these pieces of information, but if you only remember one thing, make it this: `content` contains the actual model-generated content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
